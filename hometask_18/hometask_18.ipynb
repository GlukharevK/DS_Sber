{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aCTPMHfb04dm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8JXzZfWg04do"
      },
      "outputs": [],
      "source": [
        "data_dir = 'C:/Users/glukh/JupyterFiles/SBER_школа_DS/lesson 18/'\n",
        "train_lang = 'en'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "v2eD2Nwf04dp"
      },
      "outputs": [],
      "source": [
        "def collate_fn(input_data):\n",
        "    data = []\n",
        "    chars = []\n",
        "    targets = []\n",
        "    max_len = 0\n",
        "    for item in input_data:\n",
        "        if len(item['data']) > max_len:\n",
        "            max_len = len(item['data'])\n",
        "        data.append(torch.as_tensor(item['data']))\n",
        "        chars.append(item['char'])\n",
        "        targets.append(torch.as_tensor(item['target']))\n",
        "    chars_seq = [[torch.as_tensor([0]) for _ in range(len(input_data))] for _ in range(max_len)]\n",
        "    for j in range(len(input_data)):\n",
        "        for i in range(max_len):\n",
        "            if len(chars[j]) > i:\n",
        "                chars_seq[i][j] = torch.as_tensor(chars[j][i])\n",
        "    for j in range(max_len):\n",
        "        chars_seq[j] = pad_sequence(chars_seq[j], batch_first=True, padding_value=0)\n",
        "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
        "    return {'data': data, 'chars': chars_seq, 'target': targets}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IxY63I_d04dr"
      },
      "outputs": [],
      "source": [
        "class DatasetSeq(Dataset):\n",
        "    def __init__(self, data_dir, train_lang='en'):\n",
        "\n",
        "        with open('en.train', 'r', encoding='utf-8') as f:\n",
        "            train = f.read().split('\\n\\n')\n",
        "\n",
        "        # delete extra tag markup\n",
        "        train = [x for x in train if not '_ ' in x]\n",
        "\n",
        "        self.target_vocab = {}\n",
        "        self.word_vocab = {}\n",
        "        self.char_vocab = {}\n",
        "\n",
        "        self.encoded_sequences = []\n",
        "        self.encoded_targets = []\n",
        "        self.encoded_char_sequences = []\n",
        "        n_word = 1\n",
        "        n_target = 1\n",
        "        n_char = 1\n",
        "        for line in train:\n",
        "            sequence = []\n",
        "            target = []\n",
        "            chars = []\n",
        "            for item in line.split('\\n'):\n",
        "                if item != '':\n",
        "                    word, label = item.split(' ')\n",
        "\n",
        "                    if self.word_vocab.get(word) is None:\n",
        "                        self.word_vocab[word] = n_word\n",
        "                        n_word += 1\n",
        "                    if self.target_vocab.get(label) is None:\n",
        "                        self.target_vocab[label] = n_target\n",
        "                        n_target += 1\n",
        "                    for char in word:\n",
        "                        if self.char_vocab.get(char) is None:\n",
        "                            self.char_vocab[char] = n_char\n",
        "                            n_char += 1\n",
        "                    sequence.append(self.word_vocab[word])\n",
        "                    target.append(self.target_vocab[label])\n",
        "                    chars.append([self.char_vocab[char] for char in word])\n",
        "            self.encoded_sequences.append(sequence)\n",
        "            self.encoded_targets.append(target)\n",
        "            self.encoded_char_sequences.append(chars)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {\n",
        "            'data': self.encoded_sequences[index], # [1, 2, 3, 4, 6] len=5\n",
        "            'char': self.encoded_char_sequences[index],# [[1,2,3], [4,5], [1,2], [2,6,5,4], []] len=5\n",
        "            'target': self.encoded_targets[index], #  (1)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rp0YnPKhSYkI"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "dataset = DatasetSeq(data_dir)\n",
        "data_train, data_test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "train_loader = DataLoader(\n",
        "        dataset=data_train,\n",
        "        collate_fn=collate_fn,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "test_loader = DataLoader(\n",
        "        dataset=data_test,\n",
        "        collate_fn=collate_fn,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "data_loaders = {'train' : train_loader, 'test' : test_loader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gO0ihzXM04du"
      },
      "outputs": [],
      "source": [
        "#hyper params\n",
        "vocab_len = len(dataset.word_vocab) + 1\n",
        "n_classes = len(dataset.target_vocab) + 1\n",
        "n_chars = len(dataset.char_vocab) + 1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3m9v0Y1K04dv"
      },
      "outputs": [],
      "source": [
        "class CharModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 emb_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.char_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x): # B x T\n",
        "        x = self.char_emb(x)  # B x T x V\n",
        "        _, out = self.rnn(x)\n",
        "\n",
        "        return out # B x 1 x V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "csRJAYhJ04dv"
      },
      "outputs": [],
      "source": [
        "class GRU_predictor_Chars(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 emb_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 n_classes: int,\n",
        "                 n_chars: int,\n",
        "                 char_emb_dim: int,\n",
        "                 char_hidden_dim: int,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.GRU(input_size=emb_dim+char_emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.char_model = CharModel(n_chars, char_emb_dim, char_hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x, char_seq): # B x T\n",
        "        x = self.word_emb(x)  # B(размер батча) x T(длина предложения после паддинга) x V(размерность эмбединга)\n",
        "        chars = [self.char_model(item.to(x.device)).squeeze().unsqueeze(1) for item in char_seq] # T x 1\n",
        "        chars = torch.cat(chars, dim=1)\n",
        "        rnn_out, _ = self.rnn(torch.cat([x, chars], dim=-1))\n",
        "        out = self.classifier(rnn_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tZoZD6Ag8Uwa"
      },
      "outputs": [],
      "source": [
        "class RNN_predictor_Chars(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 emb_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 n_classes: int,\n",
        "                 n_chars: int,\n",
        "                 char_emb_dim: int,\n",
        "                 char_hidden_dim: int,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.RNN(input_size=emb_dim+char_emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.char_model = CharModel(n_chars, char_emb_dim, char_hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x, char_seq): # B x T\n",
        "        x = self.word_emb(x)  # B(размер батча) x T(длина предложения после паддинга) x V(размерность эмбединга)\n",
        "        chars = [self.char_model(item.to(x.device)).squeeze().unsqueeze(1) for item in char_seq] # T x 1\n",
        "        chars = torch.cat(chars, dim=1)\n",
        "        rnn_out, _ = self.rnn(torch.cat([x, chars], dim=-1))\n",
        "        out = self.classifier(rnn_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SEFlu-fi8Uwa"
      },
      "outputs": [],
      "source": [
        "class LSTM_predictor_Chars(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 emb_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 n_classes: int,\n",
        "                 n_chars: int,\n",
        "                 char_emb_dim: int,\n",
        "                 char_hidden_dim: int,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.LSTM(input_size=emb_dim+char_emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.char_model = CharModel(n_chars, char_emb_dim, char_hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x, char_seq): # B x T\n",
        "        x = self.word_emb(x)  # B(размер батча) x T(длина предложения после паддинга) x V(размерность эмбединга)\n",
        "        chars = [self.char_model(item.to(x.device)).squeeze().unsqueeze(1) for item in char_seq] # T x 1\n",
        "        chars = torch.cat(chars, dim=1)\n",
        "        rnn_out, _ = self.rnn(torch.cat([x, chars], dim=-1))\n",
        "        out = self.classifier(rnn_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "i5oh1GiX8Uwb"
      },
      "outputs": [],
      "source": [
        "class LSTMBD_predictor_Chars(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 emb_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 n_classes: int,\n",
        "                 n_chars: int,\n",
        "                 char_emb_dim: int,\n",
        "                 char_hidden_dim: int,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.LSTM(input_size=emb_dim+char_emb_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.char_model = CharModel(n_chars, char_emb_dim, char_hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x, char_seq): # B x T\n",
        "        x = self.word_emb(x)  # B(размер батча) x T(длина предложения после паддинга) x V(размерность эмбединга)\n",
        "        chars = [self.char_model(item.to(x.device)).squeeze().unsqueeze(1) for item in char_seq] # T x 1\n",
        "        chars = torch.cat(chars, dim=1)\n",
        "        rnn_out, _ = self.rnn(torch.cat([x, chars], dim=-1))\n",
        "        out = self.classifier(rnn_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "q1iA-E_cEc1M"
      },
      "outputs": [],
      "source": [
        "def train_eval_loop(model, optim, loss_func, num_epoches, train_loader, test_loader):\n",
        "    \n",
        "    \n",
        "    accum_train_time = 0\n",
        "    accum_test_time = 0\n",
        "    for epoch in range(num_epoches):\n",
        "        for k, loader in data_loaders.items():\n",
        "            \n",
        "            accum_f1 = 0\n",
        "            accum_loss = 0\n",
        "            steps = 0\n",
        "            \n",
        "            if k == 'train':\n",
        "                start_train_time = time.time()\n",
        "                model.train()\n",
        "                \n",
        "                for step, batch in enumerate(loader):\n",
        "                    optim.zero_grad()\n",
        "                    data = batch['data'].to(device)  # B x T\n",
        "                    pred = model(data, batch['chars'])\n",
        "                    loss = loss_func(pred.view(-1, n_classes), batch['target'].view(-1).to(device))\n",
        "                    loss.backward()\n",
        "                    optim.step()\n",
        "                    accum_f1 += f1_score(batch['target'].view(-1), torch.max(pred, -1)[1].view(-1).detach().cpu(), average='macro')\n",
        "                    steps += 1\n",
        "                accum_train_time += round((time.time() - start_train_time), 2)\n",
        "                f1 = accum_f1 / steps\n",
        "                print(f'Loader: {k}, Эпоха: {epoch}, loss: {loss}, f1_score: {f1},' \n",
        "                  f'Время обучения: {accum_train_time}')\n",
        "\n",
        "            else:\n",
        "                start_eval_time = time.time()\n",
        "                for step, batch in enumerate(loader):\n",
        "                    model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        data = batch['data'].to(device)\n",
        "                        pred = model(data, batch['chars'])\n",
        "                        loss = loss_func(pred.view(-1, n_classes), batch['target'].view(-1).to(device))\n",
        "                        accum_f1 += f1_score(batch['target'].view(-1), torch.max(pred, -1)[1].view(-1).detach().cpu(), average='macro') \n",
        "                        steps += 1\n",
        "                accum_test_time += round((time.time() - start_eval_time), 2)\n",
        "                f1 = accum_f1 / steps\n",
        "                print(f'Loader: {k}, Эпоха: {epoch}, loss: {loss}, f1_score: {f1},' \n",
        "                  f'Время предсказания {accum_test_time}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibRKn8r-8Uwc"
      },
      "source": [
        "### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "huoAKcqG04dw"
      },
      "outputs": [],
      "source": [
        "gru_model = GRU_predictor_Chars(vocab_len, 200, 256, n_classes, n_chars, 32, 32)\n",
        "gru_model.train()\n",
        "gru_model = gru_model.to(device)\n",
        "gru_optim = torch.optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "gru_loss_func = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLPTV62U4w53",
        "outputId": "b565964b-3047-4173-d5c0-2676a9392b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader: train, Эпоха: 0, loss: 0.20330633223056793, f1_score: 0.4525185900856127,Время обучения: 118.06\n",
            "Loader: test, Эпоха: 0, loss: 0.18548984825611115, f1_score: 0.6405795238393539,Время предсказания 9.39\n",
            "Loader: train, Эпоха: 1, loss: 0.14182676374912262, f1_score: 0.7168275362167543,Время обучения: 235.35000000000002\n",
            "Loader: test, Эпоха: 1, loss: 0.09784117341041565, f1_score: 0.7635074337201269,Время предсказания 18.69\n",
            "Loader: train, Эпоха: 2, loss: 0.06826258450746536, f1_score: 0.794123342628257,Время обучения: 350.86\n",
            "Loader: test, Эпоха: 2, loss: 0.07977218925952911, f1_score: 0.7925495490144521,Время предсказания 28.44\n",
            "Loader: train, Эпоха: 3, loss: 0.07260090112686157, f1_score: 0.8330550912896493,Время обучения: 465.84000000000003\n",
            "Loader: test, Эпоха: 3, loss: 0.1072503924369812, f1_score: 0.8233202410819664,Время предсказания 37.84\n"
          ]
        }
      ],
      "source": [
        "train_eval_loop(gru_model, gru_optim, gru_loss_func, 4, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTiRkO9W8Uwh"
      },
      "source": [
        "### RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "wD96Hw2E04dx"
      },
      "outputs": [],
      "source": [
        "rnn_model = RNN_predictor_Chars(vocab_len, 200, 256, n_classes, n_chars, 32, 32)\n",
        "rnn_model.train()\n",
        "rnn_model = rnn_model.to(device)\n",
        "rnn_optim = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "rnn_loss_func = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uyuHSz28Uwi",
        "outputId": "64f55b30-7a99-4492-e43e-20fda2616838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader: train, Эпоха: 0, loss: 0.18631786108016968, f1_score: 0.4613104146559024,Время обучения: 80.62\n",
            "Loader: test, Эпоха: 0, loss: 0.18278342485427856, f1_score: 0.6117998573099241,Время предсказания 7.25\n",
            "Loader: train, Эпоха: 1, loss: 0.13750745356082916, f1_score: 0.6985278272238395,Время обучения: 159.26\n",
            "Loader: test, Эпоха: 1, loss: 0.1310531347990036, f1_score: 0.7504064275772906,Время предсказания 14.45\n",
            "Loader: train, Эпоха: 2, loss: 0.09383402019739151, f1_score: 0.7880420735072994,Время обучения: 238.24\n",
            "Loader: test, Эпоха: 2, loss: 0.1275968998670578, f1_score: 0.7926618603088808,Время предсказания 21.29\n",
            "Loader: train, Эпоха: 3, loss: 0.06715215742588043, f1_score: 0.8255307682182854,Время обучения: 316.07\n",
            "Loader: test, Эпоха: 3, loss: 0.0857565701007843, f1_score: 0.8181032595384776,Время предсказания 28.53\n"
          ]
        }
      ],
      "source": [
        "train_eval_loop(rnn_model, rnn_optim, rnn_loss_func, 4, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkCdUVeo8Uwi"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "AuZobLNH8Uwj"
      },
      "outputs": [],
      "source": [
        "lstm_model = LSTM_predictor_Chars(vocab_len, 200, 256, n_classes, n_chars, 32, 32)\n",
        "lstm_model.train()\n",
        "lstm_model = gru_model.to(device)\n",
        "lstm_optim = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "lstm_loss_func = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ROoIxbl8Uwj",
        "outputId": "0079b43b-74b8-4c2c-a527-6e2b02b382fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader: train, Эпоха: 0, loss: 0.07668881863355637, f1_score: 0.8584962988414996,Время обучения: 117.38\n",
            "Loader: test, Эпоха: 0, loss: 0.05088821053504944, f1_score: 0.84009405250518,Время предсказания 9.89\n",
            "Loader: train, Эпоха: 1, loss: 0.03978373855352402, f1_score: 0.8783776469607943,Время обучения: 232.62\n",
            "Loader: test, Эпоха: 1, loss: 0.045217856764793396, f1_score: 0.8491442898865617,Время предсказания 19.53\n",
            "Loader: train, Эпоха: 2, loss: 0.05245556682348251, f1_score: 0.8970508809616826,Время обучения: 348.06\n",
            "Loader: test, Эпоха: 2, loss: 0.07575657218694687, f1_score: 0.8558113227541445,Время предсказания 29.160000000000004\n",
            "Loader: train, Эпоха: 3, loss: 0.030374962836503983, f1_score: 0.911372747450588,Время обучения: 462.5\n",
            "Loader: test, Эпоха: 3, loss: 0.05595991387963295, f1_score: 0.8551675253439428,Время предсказания 39.150000000000006\n"
          ]
        }
      ],
      "source": [
        "train_eval_loop(lstm_model, lstm_optim, lstm_loss_func, 4, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3bhbwUl8Uwj"
      },
      "source": [
        "### LSTM BD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8q4W0YcH8Uwk"
      },
      "outputs": [],
      "source": [
        "lstmbd_model =LSTMBD_predictor_Chars(vocab_len, 200, 256, n_classes, n_chars, 32, 32)\n",
        "lstmbd_model.train()\n",
        "lstmbd_model = gru_model.to(device)\n",
        "lstmbd_optim = torch.optim.Adam(lstmbd_model.parameters(), lr=0.001)\n",
        "lstmbd_loss_func = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHxzSTj98Uwk",
        "outputId": "513c14c9-66f0-47a3-f31e-8f11be0c671f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader: train, Эпоха: 0, loss: 0.021105121821165085, f1_score: 0.9281630614722236,Время обучения: 116.0\n",
            "Loader: test, Эпоха: 0, loss: 0.08570992946624756, f1_score: 0.8580649131113702,Время предсказания 9.47\n",
            "Loader: train, Эпоха: 1, loss: 0.027329375967383385, f1_score: 0.9423272670407234,Время обучения: 231.19\n",
            "Loader: test, Эпоха: 1, loss: 0.06862616539001465, f1_score: 0.8646411978670722,Время предсказания 19.36\n",
            "Loader: train, Эпоха: 2, loss: 0.019916413351893425, f1_score: 0.9552020515687298,Время обучения: 346.67\n",
            "Loader: test, Эпоха: 2, loss: 0.07314936071634293, f1_score: 0.8678114240852336,Время предсказания 29.259999999999998\n",
            "Loader: train, Эпоха: 3, loss: 0.01105126366019249, f1_score: 0.9627370124732679,Время обучения: 462.87\n",
            "Loader: test, Эпоха: 3, loss: 0.12195298820734024, f1_score: 0.8657059191300963,Время предсказания 38.98\n"
          ]
        }
      ],
      "source": [
        "train_eval_loop(lstm_model, lstm_optim, lstm_loss_func, 4, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSHKsKQg8Uwk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "hometask_18.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}